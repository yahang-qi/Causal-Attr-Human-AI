{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blameworthiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df_cnn = pd.read_csv('./results_cnn.csv')\n",
    "df_resnet = pd.read_csv('./results_resnet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract probabilities\n",
    "cnn_probs = df_cnn['Prob_1_PNEUMONIA']\n",
    "resnet_probs = df_resnet['Prob_1_PNEUMONIA']\n",
    "ground_truth = df_cnn['Ground Truth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build decision-making systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds\n",
    "uncertainty_threshold = 0.3  # Model 2 is considered certain if probability is > 0.7 or < 0.3\n",
    "decision_threshold = 0.5     # Threshold for making a binary decision\n",
    "\n",
    "# System 1: Direct decision from Model 1 based on its probability\n",
    "decisions_human_only = (resnet_probs > decision_threshold).astype(int)\n",
    "\n",
    "# System 2: If Model 2's probability is certain (outside [0.3, 0.7]), use its decision, otherwise use Model 1\n",
    "decisions_HITL = np.where(\n",
    "    (cnn_probs > (1-uncertainty_threshold)) | (cnn_probs < uncertainty_threshold),  # Model 2 is certain\n",
    "    (cnn_probs > decision_threshold).astype(int),       # Use Model 2's decision\n",
    "    decisions_human_only                                              # Otherwise, fallback to Model 1's decision\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate decision-making system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for Human-Only system: 0.8960739030023095\n",
      "F1 score for HITL system: 0.8311965811965811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Compute AUC for both systems\n",
    "f1_human = f1_score(ground_truth, decisions_human_only)\n",
    "f1_HITL = f1_score(ground_truth, decisions_HITL)\n",
    "\n",
    "print(f\"F1 score for Human-Only system: {f1_human}\")\n",
    "print(f\"F1 score for HITL system: {f1_HITL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that incorporating AI (CNN) resulted in drop of performance of the system. So the action of deploying such human-AI system is blameworthy against the action of deploying human-only system. However, the degree of blameworthiness can be discounted by the improvement of efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inevitable Errors (flagged by AI): 2\n",
      "Inevitable Errors (not flagged by AI): 83\n",
      "Avoidable Errors: 73\n"
     ]
    }
   ],
   "source": [
    "# Inevitable Errors: System 2 errors where Model 1 is also wrong\n",
    "inevitable_errors = (decisions_HITL != ground_truth) & (decisions_human_only != ground_truth)\n",
    "\n",
    "# Flagged by Model 2: Model 2 was uncertain, so System 2 fell back to Model 1's decision\n",
    "flagged_by_AI = (cnn_probs >= 0.3) & (cnn_probs <= 0.7)\n",
    "\n",
    "# Inevitable Errors that were flagged by Model 2\n",
    "inevitable_flagged = inevitable_errors & flagged_by_AI\n",
    "\n",
    "# Inevitable Errors that were not flagged by Model 2 (Model 2 was certain and used)\n",
    "inevitable_not_flagged = inevitable_errors & ~flagged_by_AI\n",
    "\n",
    "# Avoidable Errors: System 2 made an error but Model 1 was correct\n",
    "avoidable_errors = (decisions_HITL != ground_truth) & (decisions_human_only == ground_truth)\n",
    "\n",
    "# Count the errors\n",
    "inevitable_flagged_count = np.sum(inevitable_flagged)\n",
    "inevitable_not_flagged_count = np.sum(inevitable_not_flagged)\n",
    "avoidable_errors_count = np.sum(avoidable_errors)\n",
    "\n",
    "print(f\"Inevitable Errors (flagged by AI): {inevitable_flagged_count}\")\n",
    "print(f\"Inevitable Errors (not flagged by AI): {inevitable_not_flagged_count}\")\n",
    "print(f\"Avoidable Errors: {avoidable_errors_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inevitable errors are errors where human (ResNet) was wrong, and they are further divided into:\n",
    "* Flagged by AI: AI was not certain and the system fell back to human. (Human bears the responsibility for this type of error)\n",
    "* Not Flagged by AI: AI was certain and made a decision (Both human and AI are responsibility for this type of error). \n",
    "\n",
    "Avoidable errors refer to errors that could have been avoided if AI has requested intervention from human (Both AI and the party responsible for flagging mechanism are accountable for errors)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CausalRespAttrib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
